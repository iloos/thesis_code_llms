%## New Introduction with Polished Content and Latex:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%		~~~~ Introduction ~~~~
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\label{chap:intro}
\pagestyle{fancy}

\section{Background and Evolution of Large Language Models (LLMs)}

Developments for Large language models (LLMs) have gone very fast since the release of chatGPT. These models replicate human-like capabilities, such as generating fluent sentences in various natural languages and seemingly reasoning about complex tasks. This progress is attributed to the development of large neural networks and the availability of vast amounts of training data. LLMs exhibit potential in various domains, including interpreting and generating text, and one such domain is programming. 

Code serves as training data for LLMs, leading to a new category of capable coding LLMs. These models find applications in numerous software engineering tasks, with the collection of available models constantly expanding. 

This paper investigates how to compare different LLMs, such as GPT-3, GPT-4 \cite{openai2023gpt4}, Gemini \cite{geminiteam2024gemini}, CodeLLama \cite{rozière2024code}, and Github Copilot \cite{dakhel2023github}. Human-coding-AI interactions encompass a variety of tasks, from understanding documentation and generating code from natural language descriptions to auto-completing existing code \cite{fan2023large}.

\section{Significance of LLMs in Natural Language Processing and Code Generation}

This section will explore the significance of LLMs in Natural Language Processing (NLP) and Code Generation. Here, you can discuss the impact of LLMs on various NLP tasks and how they specifically contribute to code generation. 

\section{Research Objectives and Scope}

\begin{enumerate}
  \item What are the different strengths and weaknesses between different coding LLMs?
    \begin{enumerate}
      \item What metrics can we use to evaluate different LLMs?
      \item How do current LLM evaluation frameworks for code generation and datasets work, and what are the shortcomings?
    \end{enumerate}
  \item How will the future of software development be impacted by coding LLMs?
    \begin{enumerate}
      \item How will software developers be impacted by AI code generation?
      \item How will other trends like multi-agent systems (Devin) affect the paradigm of coding LLMs?
    \end{enumerate}
\end{enumerate}

\section{Structure of the Thesis}
besides the table of contents, this thesis can be divided into three main parts:

\textbf{Part 1: Fundamentals of large language models}

We start by going over the foundational concepts of LLMs, looking at their design and training set. We'll explore the ways in which state-of-the-art (SOTA) LLMs are specially designed to comprehend and produce Python code.

\textbf{Part 2: Unveiling the Performance Landscape: Evaluating LLM Efficacy}

It's still difficult to assess and evaluate LLM-generated code. The benchmarks currently in use for evaluating LLM performance are examined critically in this section.  We will report on our findings about the performance of top LLMs on some benchmarks, offering a useful overview of their present strengths and weaknesses in Python code generation. Alternative new methods of evaluation will also be mentioned.

\textbf{Part 3: The Future Landscape of Software Development}

In the last section, we will examine how LLMs can influence software development processes, both currently and in the future. We'll talk about new developments that could further change this industry, such multi-agent coding systems.  

Often the ethical implications and potential consequences aren’t discussed in the field of AI research. So, we will wrap up by talking about the ethical issues and societal ramifications of LLM use in software development.

\section{Challenges in Evaluating Code Generated by LLMs} 

The quality of LLM-generated code is challenging to evaluate quantitatively. The model needs to be suitable for diverse applications with varying requirements. For instance, generating code for data visualization tasks might be suitable for a data analyst, but refactoring code for safety and optimization is a more complex evaluation challenge \cite{fan2023large, peng2023impact}. 

There is currently no single, standardized test to compare LLM performance. This paper does not aim to create one, but by discussing the multifaceted nature of LLM evaluation, it hopes to empower industry professionals and researchers to better understand and assess various code-generating LLMs. We will discuss the most popular evaluation benchmarks, such as HumanEval.\cite{chen2021evaluating}

The most common benchmark involves generating code snippets based on natural language descriptions of tasks. However, other formats of coding LLMs exist, including code explanation and autocompletion, which require their own models \cite{fan2023large}.

Beyond effectiveness and reliability, other factors are crucial for comparing models, such as price and ease of use, which have received less attention in the literature. 

While developers report increased perceived productivity with LLMs, the long-term effects remain unclear.\cite{codingoncopilot2023} GitHub claims Copilot facilitates "55\% faster" code writing. However, should code be written at all if unnecessary? Robert Martin, author of "Clean Code: A Handbook of Agile Software Craftsmanship," highlights that code is generally read 10 times more than it's written. We will study to what extent this may influence the field of software development.


This paper aims to provide a comprehensive overview of different aspects to consider when comparing LLMs.  It will also present a compilation of results for a general understanding of current leading LLMs for code generation. Following this foundation, the paper will explore the impact of LLMs on software development practices.



\textbf{Overarching Research Questions:}

\begin{enumerate}
  \item How do different large language models (LLMs) compare in their ability to generate Python code, and what factors contribute to their performance variability?
  \item What are the implications of coding LLMs for the future of software development practices, and how can potential risks be mitigated while maximizing their benefits?
\end{enumerate}